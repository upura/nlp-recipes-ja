# Embeddings

This folder contains examples for getting pretrained embedding vectors.

## What is Word Embedding?

>Word embedding is a technique to map words or phrases from a vocabulary to vectors or real numbers.
>The learned vector representations of words capture  syntactic and semantic word relationships and therefore can be very useful for  tasks like sentence similary, text classifcation, etc.

https://github.com/microsoft/nlp-recipes/blob/master/examples/embeddings/README.md

## Japanese pretrained models

There is a survey article titled "[学習済み日本語word2vecとその評価について](https://blog.hoxo-m.com/entry/2020/02/20/090000)". This article introduces many Japanese pretrained embedding models avaliable and evaluate them.

## Summary

|Notebook|Environment|Description| 
|---|---|---|
|[Word2vec](get_word2vec.py)|Local| Get [word2vec vectors pretrained by Japanese Wikipedia](https://qiita.com/Hironsan/items/513b9f93752ecee9e670) |
|[Download Pre-trained Embeddings](download_embeddings.py)|Local| Download pre-trained embeddings by [chakin](https://github.com/chakki-works/chakin) |
